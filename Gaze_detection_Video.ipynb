{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\User/.cache\\torch\\hub\\fkryan_gazelle_main\n",
      "c:\\Users\\User\\.conda\\envs\\deepl\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using cache found in C:\\Users\\User/.cache\\torch\\hub\\facebookresearch_dinov2_main\n",
      "C:\\Users\\User/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "C:\\Users\\User/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "C:\\Users\\User/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "# === STEP 1: Load Models ===\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# Load YOLOv8 custom-trained head detector\n",
    "head_detector = YOLO(\"custom_head_yolov8n.pt\")\n",
    "\n",
    "# Load Gaze-LLE model and transform\n",
    "from torch.hub import load\n",
    "\n",
    "gaze_model, gaze_transform = load('fkryan/gazelle', 'gazelle_dinov2_vitb14')\n",
    "gaze_model.eval()\n",
    "\n",
    "\n",
    "# === STEP 2: Detect Heads ===\n",
    "def detect_heads_from_frame(frame):\n",
    "    results = head_detector.predict(source=frame, stream=False, verbose=False)\n",
    "    boxes = []\n",
    "    result = results[0]\n",
    "    for box in result.boxes:\n",
    "        if int(box.cls[0]) == 0:\n",
    "            x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "            boxes.append((x1, y1, x2, y2))\n",
    "    return boxes\n",
    "\n",
    "\n",
    "# === STEP 3: Run Gaze-LLE ===\n",
    "def run_gaze_lle_on_frame(pil_frame, head_bboxes):\n",
    "    width, height = pil_frame.size\n",
    "    input_tensor = gaze_transform(pil_frame).unsqueeze(0)\n",
    "\n",
    "    norm_bboxes = [\n",
    "        (x1 / width, y1 / height, x2 / width, y2 / height)\n",
    "        for (x1, y1, x2, y2) in head_bboxes\n",
    "    ]\n",
    "\n",
    "    if not norm_bboxes:\n",
    "        return []\n",
    "\n",
    "    input_data = {'images': input_tensor, 'bboxes': [norm_bboxes]}\n",
    "    with torch.no_grad():\n",
    "        output = gaze_model(input_data)\n",
    "\n",
    "    return output['heatmap'][0]  # shape: [num_heads, H, W]\n",
    "\n",
    "\n",
    "# === STEP 4: Draw on Frame ===\n",
    "from PIL import ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "def draw_gaze_on_frame(pil_frame, head_bboxes, heatmaps):\n",
    "    draw = ImageDraw.Draw(pil_frame)\n",
    "    width, height = pil_frame.size\n",
    "\n",
    "    for (x1, y1, x2, y2), heatmap in zip(head_bboxes, heatmaps):\n",
    "        head_center = ((x1 + x2) / 2, (y1 + y2) / 2)\n",
    "\n",
    "        heatmap_np = heatmap.detach().cpu().numpy()\n",
    "        y, x = np.unravel_index(np.argmax(heatmap_np), heatmap_np.shape)\n",
    "        gaze_x = x * width / heatmap_np.shape[1]\n",
    "        gaze_y = y * height / heatmap_np.shape[0]\n",
    "        gaze_target = (gaze_x, gaze_y)\n",
    "\n",
    "        draw.rectangle([(x1, y1), (x2, y2)], outline=\"red\", width=2)\n",
    "        draw.line([head_center, gaze_target], fill=\"blue\", width=2)\n",
    "        draw.ellipse((gaze_x - 3, gaze_y - 3, gaze_x + 3, gaze_y + 3), fill=\"blue\")\n",
    "\n",
    "    return pil_frame\n",
    "\n",
    "\n",
    "# === STEP 5: Process Video ===\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "def process_video(video_path, save_path=None):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = None\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert to PIL image\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pil_frame = Image.fromarray(rgb_frame)\n",
    "\n",
    "        # Run detection and gaze\n",
    "        head_bboxes = detect_heads_from_frame(pil_frame)\n",
    "        if not head_bboxes:\n",
    "            continue\n",
    "        heatmaps = run_gaze_lle_on_frame(pil_frame, head_bboxes)\n",
    "        result_frame = draw_gaze_on_frame(pil_frame, head_bboxes, heatmaps)\n",
    "\n",
    "        # Convert back to OpenCV format\n",
    "        result_np = np.array(result_frame)\n",
    "        result_bgr = cv2.cvtColor(result_np, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if save_path:\n",
    "            if out is None:\n",
    "                h, w, _ = result_bgr.shape\n",
    "                out = cv2.VideoWriter(save_path, fourcc, 20.0, (w, h))\n",
    "            out.write(result_bgr)\n",
    "        else:\n",
    "            cv2.imshow(\"Gaze Estimation\", result_bgr)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    if out:\n",
    "        out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\.conda\\envs\\deepl\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "process_video(\"input_gaze.mp4\", \"output_gaze.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
